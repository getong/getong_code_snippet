* renegade code reading

** openraft raft start

#+begin_src rust
/// A client interface to the raft
#[derive(Clone)]
pub struct RaftClient {
    /// The client's config
    pub(crate) config: RaftClientConfig,
    /// The inner raft
    raft: Raft,
    /// The network to use for the raft client
    network_factory: P2PNetworkFactoryWrapper,
    /// A lock to prevent simultaneous membership changes
    membership_change_lock: Arc<Mutex<()>>,
}

impl RaftClient {
  /// Create a new raft client
  pub async fn new<N: P2PNetworkFactory>(
    config: RaftClientConfig,
    db: Arc<DB>,
    net_factory: N,
    state_machine: StateMachine,
  ) -> Result<Self, ReplicationV2Error> {
    let raft_config = Arc::new(RaftConfig {
      cluster_name: config.cluster_name.clone(),
      heartbeat_interval: config.heartbeat_interval,
      election_timeout_min: config.election_timeout_min,
      election_timeout_max: config.election_timeout_max,
      snapshot_max_chunk_size: config.snapshot_max_chunk_size,
      install_snapshot_timeout: config.install_snapshot_timeout,
      max_payload_entries: config.max_payload_entries,
      ..Default::default()
    });

    // Create the raft
    let p2p_factory = P2PNetworkFactoryWrapper::new(net_factory);
    let log_store = LogStore::new(db.clone());
    let raft = Raft::new(config.id, raft_config, p2p_factory.clone(), log_store, state_machine)
      .await
      .map_err(err_str!(ReplicationV2Error::RaftSetup))?;

    // Initialize the raft
    if config.init {
      let initial_nodes = config.initial_nodes.clone();
      let members = initial_nodes.into_iter().collect::<BTreeMap<_, _>>();
      raft.initialize(members).await.map_err(err_str!(ReplicationV2Error::RaftSetup))?;
    }

    Ok(Self {
      config,
      raft,
      network_factory: p2p_factory,
      membership_change_lock: Arc::new(Mutex::new(())),
    })
  }
}
#+end_src

** P2PNetworkFactory definition

#+begin_src rust
pub trait P2PNetworkFactory: Send + Sync + 'static {
  /// Create a new p2p client
  fn new_p2p_client(&self, target: NodeId, target_info: Node) -> P2PRaftNetworkWrapper;
}

/// A wrapper around the p2p raft network that allows for a default
/// `RaftNetwork` implementation and to hide generics from higher level
/// interfaces
pub struct P2PRaftNetworkWrapper {
  /// The inner p2p network
  inner: Box<dyn P2PRaftNetwork + Send + Sync>,
}

/// A generalization of the raft network trait that specifically allows for
/// point-to-point communication
///
/// We implement the general raft network trait for all types that fit this
/// signature by simply calling out to the p2p implementation
#[async_trait]
pub trait P2PRaftNetwork: 'static + Sync + Send {
  /// The target this client is sending requests to
  fn target(&self) -> NodeId;
  /// Send an request to the target node
  async fn send_request(
    &self,
    target: NodeId,
    request: RaftRequest,
  ) -> Result<RaftResponse, RPCError<NodeId, Node, RaftError<NodeId>>>;
}

/// The request type a raft node may send to another
#[derive(Clone, Debug, Serialize, Deserialize)]
pub enum RaftRequest {
  /// A request to append entries
  AppendEntries(AppendEntriesRequest<TypeConfig>),
  /// A request to install a snapshot
  InstallSnapshot(InstallSnapshotRequest<TypeConfig>),
  /// A request to vote
  Vote(VoteRequest<NodeId>),
  /// A proposal forwarded to the leader
  ForwardedProposal(Proposal),
}

/// The response type a raft node may send to another
#[derive(Debug, Serialize, Deserialize)]
pub enum RaftResponse {
  /// A simple ack when no data must be returned
  Ack,
  /// A response to an append entries request
  AppendEntries(AppendEntriesResponse<NodeId>),
  /// A response to an install snapshot request
  InstallSnapshot(Result<InstallSnapshotResponse<NodeId>, InstallSnapshotError>),
  /// A response to a vote request
  Vote(VoteResponse<NodeId>),
}
#+end_src

** openraft type definition

#+begin_src rust
// Declare the types config for the raft
openraft::declare_raft_types! (
    /// The type config for the raft
    pub TypeConfig:
    D = Proposal,
    R = (), // Response
    Node = RaftNode,
    SnapshotData = tokio::fs::File,
);

/// A type alias for the node id type
pub type NodeId = <TypeConfig as RaftTypeConfig>::NodeId;
/// A type alias for the node type
pub type Node = <TypeConfig as RaftTypeConfig>::Node;

/// The network shim
#[derive(Clone)]
pub struct GossipNetwork {
    /// The target node for this instance
    target: NodeId,
    /// The target node info
    target_info: Node,
    /// A sender to the network manager's queue
    network_sender: NetworkManagerQueue,
}

#+end_src

** libp2p code example

#+begin_src rust
use libp2p::request_response::ResponseChannel;
use libp2p_core::Multiaddr;
use tokio::sync::{
    mpsc::{unbounded_channel, UnboundedSender as TokioSender},
    oneshot::{channel as oneshot_channel, Receiver as OneshotReceiver, Sender as OneshotSender},
};

/// The job type for the network manager
#[derive(Debug)]
pub enum NetworkManagerJob {
    /// Send an outbound pubsub message
    ///
    /// The first field is the topic, the second is the message body
    Pubsub(String, PubsubMessage),
    /// Send a gossip request
    ///
    /// Optionally, the sending worker may specify a channel to receive the
    /// corresponding gossip response on
    Request(WrappedPeerId, GossipRequest, Option<NetworkResponseChannel>),
    /// Send a gossip response
    Response(GossipResponse, ResponseChannel<AuthenticatedGossipResponse>),
    /// An internal networking directive
    Internal(NetworkManagerControlSignal),
}

/// The task queue type for the network manager
pub type NetworkManagerQueue = TokioSender<NetworkManagerJob>;
/// The task queue receiver type for the network manager
pub type NetworkManagerReceiver = MeteredTokioReceiver<NetworkManagerJob>;

/// The composed behavior that handles all types of network requests that
/// various workers need access to
#[derive(NetworkBehaviour)]
#[behaviour(out_event = "ComposedProtocolEvent")]
pub struct ComposedNetworkBehavior {
    /// The request/response behavior; provides a point-to-point communication
    /// primitive for relayers to dial each other directly on
    pub request_response: RequestResponse<RelayerGossipCodec>,
    /// The Kademlia DHT behavior; used for storing distributed state, including
    /// peer address information
    pub kademlia_dht: Kademlia<MemoryStore>,
    /// The Gossipsub behavior; used for broadcast (pubsub) primitives
    pub pubsub: Gossipsub,
    /// The identify protocol behavior, used for getting publicly facing
    /// information about the local node
    pub identify: IdentifyProtocol,
}

fn start(&mut self) -> Result<(), Self::Error> {
  // Build a quic transport
  let hostport = format!("/ip4/{}/udp/{}/quic-v1", self.config.bind_addr, self.config.port);
  let addr: Multiaddr = hostport.parse().unwrap();

  // Build the quic transport
  let config = QuicConfig::new(&self.local_keypair);
  let quic_transport = QuicTransport::new(config)
    .map(|(peer_id, quic_conn), _| (peer_id, StreamMuxerBox::new(quic_conn)))
    .boxed();

  // Defines the behaviors of the underlying networking stack: including gossip,
  // pubsub, address discovery, etc
  let mut behavior = ComposedNetworkBehavior::new(
    ,*self.local_peer_id,
    ProtocolVersion::Version0,
    &self.local_keypair,
  )?;

  // Add any bootstrap addresses to the peer info table
  let peer_index = block_on(self.config.clone().global_state.get_peer_info_map())?;
  for (peer_id, peer_info) in peer_index.iter() {
    info!("Adding {:?}: {} to routing table...", peer_id, peer_info.get_addr());
    behavior.kademlia_dht.add_address(peer_id, peer_info.get_addr());
  }

  // Connect the behavior and the transport via swarm and enter the network
  let mut swarm =
    SwarmBuilder::with_tokio_executor(quic_transport, behavior, *self.local_peer_id)
    .build();
  swarm.listen_on(addr).map_err(|err| NetworkManagerError::SetupError(err.to_string()))?;
}
#+end_src

** NetworkManagerExecutor implementation

#+begin_src rust
impl NetworkManagerExecutor {
    /// Enqueue a behavior job from elsewhere in the network manager
    pub(crate) fn send_behavior(&self, job: BehaviorJob) -> Result<(), NetworkManagerError> {
        self.behavior_tx.send(job).map_err(err_str!(NetworkManagerError::EnqueueJob))
    }

    /// Handle a behavior job
    #[instrument(name = "handle_behavior_job", skip_all)]
    pub(crate) async fn handle_behavior_job(
        &mut self,
        job: BehaviorJob,
        swarm: &mut Swarm<ComposedNetworkBehavior>,
    ) -> Result<(), NetworkManagerError> {
        match job {
            BehaviorJob::SendReq(peer_id, req, chan) => {
                set_parent_span_from_headers(&req.inner.tracing_headers());

                let rid = swarm.behaviour_mut().request_response.send_request(&peer_id, req);
                if let Some(chan) = chan {
                    self.response_waiters.insert(rid, chan).await;
                }

                Ok(())
            },
            // ....
        }
    }
}
#+end_src

** event loop

#+begin_src rust
let thread_handle = Builder::new()
    .name("network-manager-main-loop".to_string())
    .spawn(move || {
        // Build a tokio runtime for the network manager
        let runtime = TokioRuntimeBuilder::new_multi_thread()
            .worker_threads(NETWORK_MANAGER_N_THREADS)
            .enable_all()
            .build()
            .expect("building a runtime to the network manager failed");

        // Block on this to execute the future in a separate thread
        runtime.block_on(executor.executor_loop(swarm))
    })
    .map_err(|err| NetworkManagerError::SetupError(err.to_string()))?;

pub async fn executor_loop(
    mut self,
    mut swarm: Swarm<ComposedNetworkBehavior>,
) -> NetworkManagerError {
    info!("Starting executor loop for network manager...");
    let mut cancel_channel = self.cancel.take().unwrap();
    let mut job_channel = self.job_channel.take().unwrap();
    let mut behavior_channel = self.behavior_rx.take().unwrap();

    loop {
        tokio::select! {
            // Handle behavior requests from inside the worker
            Some(behavior_request) = behavior_channel.recv() => {
                if let Err(err) = self.handle_behavior_job(behavior_request, &mut swarm).await {
                    error!("Error handling behavior job: {err}");
                }
            },

            // Handle network requests from worker components of the relayer
            Some(job) = job_channel.recv() => {
                // Forward the message
                let this = self.clone();
                tokio::spawn(async move {
                    if let Err(err) = this.handle_job(job).await {
                        error!("Error sending outbound message: {err}");
                    }
                });
            },

            // Handle network events and dispatch
            event = swarm.select_next_some() => {
                match event {
                    SwarmEvent::Behaviour(event) => {
                        let this = self.clone();
                        tokio::spawn(async move {
                            if let Err(err) = this.handle_inbound_message(event).await {
                                error!("error in network manager: {:?}", err);
                            }
                        });
                    },
                    SwarmEvent::NewListenAddr { address, .. } => {
                        info!("Listening on {}/p2p/{}\n", address, self.local_peer_id);
                    },
                    // This catchall may be enabled for fine-grained libp2p introspection
                    x => { debug!("Unhandled swarm event: {:?}", x) }
                }
            }

            // Handle a cancel signal from the coordinator
            _ = cancel_channel.changed() => {
                return NetworkManagerError::Cancelled("received cancel signal".to_string())
            }
        }
    }
}

pub(crate) async fn handle_behavior_job(
    &mut self,
    job: BehaviorJob,
    swarm: &mut Swarm<ComposedNetworkBehavior>,
) -> Result<(), NetworkManagerError> {
    match job {
        BehaviorJob::SendReq(peer_id, req, chan) => {
            set_parent_span_from_headers(&req.inner.tracing_headers());

            let rid = swarm.behaviour_mut().request_response.send_request(&peer_id, req);
            if let Some(chan) = chan {
                self.response_waiters.insert(rid, chan).await;
            }

            Ok(())
        },
        BehaviorJob::SendResp(channel, resp) => {
            set_parent_span_from_headers(&resp.inner.tracing_headers());

            swarm
                .behaviour_mut()
                .request_response
                .send_response(channel, resp)
                .map_err(|_| NetworkManagerError::Network(ERR_SEND_RESPONSE.to_string()))
        },
        BehaviorJob::SendPubsub(topic, msg) => swarm
            .behaviour_mut()
            .pubsub
            .publish(topic, msg)
            .map(|_| ())
            .map_err(err_str!(NetworkManagerError::Network)),
        BehaviorJob::AddAddress(peer_id, addr) => {
            swarm.behaviour_mut().kademlia_dht.add_address(&peer_id, addr);
            Ok(())
        },
        BehaviorJob::RemovePeer(peer_id) => {
            swarm.behaviour_mut().kademlia_dht.remove_peer(&peer_id);
            Ok(())
        },
        BehaviorJob::LookupAddr(peer_id, sender) => {
            let addr = swarm
                .behaviour_mut()
                .handle_pending_outbound_connection(
                    ConnectionId::new_unchecked(0),
                    Some(peer_id),
                    &[],
                    Endpoint::Dialer,
                )
                .map_err(|_| NetworkManagerError::Network(ERR_NO_KNOWN_ADDR.to_string()))?;

            sender
                .send(addr)
                .map_err(|_| NetworkManagerError::SendInternal(ERR_SEND_INTERNAL.to_string()))
        },
    }
}
#+end_src

** GossipNetwork

#+begin_src rust
let (network_sender, network_receiver) = new_network_manager_queue();

/// The network shim
#[derive(Clone)]
pub struct GossipNetwork {
    /// The target node for this instance
    target: NodeId,
    /// The target node info
    target_info: Node,
    /// A sender to the network manager's queue
    network_sender: NetworkManagerQueue,
}

impl GossipNetwork {
    /// Constructor
    pub fn new(target: NodeId, target_info: Node, network_sender: NetworkManagerQueue) -> Self {
        Self { target, target_info, network_sender }
    }

    /// Construct a new `GossipNetwork` instance without target specified
    pub fn empty(network_sender: NetworkManagerQueue) -> Self {
        Self { target: NodeId::default(), target_info: Node::default(), network_sender }
    }

    /// Convert a gossip response into a raft response
    fn to_raft_response(resp: GossipResponse) -> Result<RaftResponse, ReplicationV2Error> {
        let resp_bytes = match resp.body {
            GossipResponseType::Raft(x) => x,
            _ => {
                return Err(ReplicationV2Error::Deserialize(ERR_INVALID_RESPONSE.to_string()));
            },
        };

        let raft_resp = Self::deserialize_raft_response(&resp_bytes)?;
        Ok(raft_resp)
    }

    /// Deserialize a raft response from bytes
    fn deserialize_raft_response(msg_bytes: &[u8]) -> Result<RaftResponse, ReplicationV2Error> {
        ciborium::de::from_reader(msg_bytes).map_err(err_str!(ReplicationV2Error::Deserialize))
    }
}

#[async_trait]
impl P2PRaftNetwork for GossipNetwork {
    fn target(&self) -> NodeId {
        self.target
    }

    #[allow(clippy::blocks_in_conditions)]
    #[instrument(
        name = "send_raft_request",
        skip_all, err
        fields(req_type = %request.type_str())
    )]
    async fn send_request(
        &self,
        _target: NodeId,
        request: RaftRequest,
    ) -> Result<RaftResponse, RPCError<NodeId, Node, RaftError<NodeId>>> {
        // We serialize in the raft layer to avoid the `gossip-api` depending on `state`
        let ser =
            ciborium_serialize(&request).map_err(|e| RPCError::Network(NetworkError::new(&e)))?;
        let req = GossipRequestType::Raft(ser);

        // Send a network manager job
        let peer_id = self.target_info.peer_id;
        let (job, rx) = NetworkManagerJob::request_with_response(peer_id, req);
        self.network_sender.send(job).unwrap();

        // TODO: timeout and error handling
        let resp = rx.await.unwrap();
        Self::to_raft_response(resp).map_err(new_network_error)
    }
}

impl P2PNetworkFactory for GossipNetwork {
    fn new_p2p_client(&self, target: NodeId, target_info: Node) -> P2PRaftNetworkWrapper {
        let mut clone = self.clone();
        clone.target = target;
        clone.target_info = target_info;

        P2PRaftNetworkWrapper::new(clone)
    }
}

#+end_src

_GossipNetwork_ send request to network, and work in _StateInner_ .
_send_request()_ method is used to send msg to NetworkManagerExecutor, and decode reponse via _ciborium_ crate with _to_raft_response_ method.

*** StateInner::new() method

#+begin_src rust
impl StateInner {
    // ----------------
    // | Constructors |
    // ----------------

    /// Construct a new default state handle using the `GossipNetwork`
    pub async fn new(
        config: &RelayerConfig,
        network_queue: NetworkManagerQueue,
        task_queue: TaskDriverQueue,
        handshake_manager_queue: HandshakeManagerQueue,
        system_bus: SystemBus<SystemBusMessage>,
        system_clock: &SystemClock,
        failure_send: WorkerFailureSender,
    ) -> Result<Self, StateError> {
        let raft_config = Self::build_raft_config(config);
        let net = GossipNetwork::empty(network_queue);
        Self::new_with_network(
            config,
            raft_config,
            net,
            task_queue,
            handshake_manager_queue,
            system_bus,
            system_clock,
            failure_send,
        )
            .await
    }
}
#+end_src


** job queue and notification

#+begin_src rust
/// Create a new task driver queue
pub fn new_task_driver_queue() -> (TaskDriverQueue, TaskDriverReceiver) {
    let (send, recv) = crossbeam::channel::unbounded();
    (send, MeteredCrossbeamReceiver::new(recv, TASK_DRIVER_QUEUE_NAME))
}

/// Create a new notification channel and job for the task driver
pub fn new_task_notification(task_id: TaskIdentifier) -> (TaskNotificationReceiver, TaskDriverJob) {
    let (sender, receiver) = oneshot_channel();
    (receiver, TaskDriverJob::Notify { task_id, channel: sender })
}
#+end_src


** raft request response function

#+begin_src rust
/// Propose an update to the raft
pub async fn propose_transition(&self, update: Proposal) -> Result<(), ReplicationV2Error> {
    // If the current node is not the leader, forward to the leader
    let (mut leader_nid, leader_info) = self
        .leader_info()
        .ok_or_else(|| ReplicationV2Error::Proposal(ERR_NO_LEADER.to_string()))?;

    // If we're expiring the leader, first change leader then propose an expiry
    if let StateTransition::RemoveRaftPeers { peer_ids } = update.transition.as_ref()
        && peer_ids.contains(&leader_nid)
    {
        info!("removing raft leader");
        leader_nid = self.change_leader().await?;
    }

    if leader_nid != self.node_id() {
        // Get a client to the leader's raft
        let net = self.network_factory.new_p2p_client(leader_nid, leader_info);

        // Send a message
        let msg = RaftRequest::ForwardedProposal(update);
        net.send_request(leader_nid, msg)
            .await
            .map_err(err_str!(ReplicationV2Error::Proposal))?;
        return Ok(());
    }

    match *update.transition {
        StateTransition::AddRaftLearners { learners } => {
            self.handle_add_learners(learners).await
        },
        StateTransition::AddRaftVoters { peer_ids } => self.handle_add_voters(peer_ids).await,
        StateTransition::RemoveRaftPeers { peer_ids } => {
            self.handle_remove_peers(peer_ids).await
        },
        _ => self
            .raft()
            .client_write(update)
            .await
            .map_err(err_str!(ReplicationV2Error::Proposal))
            .map(|_| ()),
    }
}
#+end_src

** create global state

#+begin_src rust
/// A handle on the state that allows workers throughout the node to access the
/// replication and durability primitives backing the state machine
pub type State = Arc<StateInner>;

/// Create a new state instance and wrap it in an `Arc`
pub async fn create_global_state(
    config: &RelayerConfig,
    network_queue: NetworkManagerQueue,
    task_queue: TaskDriverQueue,
    handshake_manager_queue: HandshakeManagerQueue,
    system_bus: SystemBus<SystemBusMessage>,
    system_clock: &SystemClock,
    failure_send: WorkerFailureSender,
) -> Result<State, StateError> {
    let state = StateInner::new(
        config,
        network_queue,
        task_queue,
        handshake_manager_queue,
        system_bus,
        system_clock,
        failure_send,
    )
    .await?;
    Ok(Arc::new(state))
}
#+end_src

_StateInner_ is the inner state of renegade.


openraft <=> tokio oneshot <=> libp2p request response
StateInner(has RaftClient) <=> GossipNetwork <=> NetworkManagerExecutor(config with NetworkManagerConfig and NetworkManager)

** entity definition

#+begin_src rust
/// The worker configuration for the network manager
#[derive(Clone)]
pub struct NetworkManagerConfig {
    /// The port to listen for inbound traffic on
    pub port: u16,
    /// The address to bind to for inbound traffic
    pub bind_addr: IpAddr,
    /// The cluster ID of the local peer
    pub cluster_id: ClusterId,
    /// Whether or not to allow discovery of peers on the localhost
    pub allow_local: bool,
    /// The cluster keypair, wrapped in an option to allow the worker thread to
    /// take ownership of the keypair
    pub cluster_symmetric_key: HmacKey,
    /// The asymmetric key of the cluster
    pub cluster_keypair: DefaultOption<Keypair>,
    /// The known public addr that the local node is listening behind, if one
    /// exists
    pub known_public_addr: Option<SocketAddr>,
    /// The channel on which to receive requests from other workers
    /// for outbound traffic
    /// This is wrapped in an option to allow the worker thread to take
    /// ownership of the work queue once it is started. The coordinator
    /// will be left with `None` after this happens
    pub send_channel: DefaultOption<NetworkManagerReceiver>,
    /// The work queue to forward inbound heartbeat requests to
    pub gossip_work_queue: GossipServerQueue,
    /// The work queue to forward inbound handshake requests to
    pub handshake_work_queue: HandshakeManagerQueue,
    /// The system bus, used to stream internal pubsub messages
    pub system_bus: SystemBus<SystemBusMessage>,
    /// The global shared state of the local relayer
    pub global_state: State,
    /// The channel on which the coordinator can send a cancel signal to
    /// all network worker threads
    pub cancel_channel: CancelChannel,
}

/// A handle on the state that allows workers throughout the node to access the
/// replication and durability primitives backing the state machine
pub type State = Arc<StateInner>;

/// The inner state struct, wrapped in an `Arc` to allow for efficient clones
#[derive(Clone)]
pub struct StateInner {
    /// The runtime config of the state
    pub(crate) config: StateConfig,
    /// The order book cache
    pub(crate) order_cache: Arc<OrderBookCache>,
    /// A handle on the database
    pub(crate) db: Arc<DB>,
    /// The system bus for sending notifications to other workers
    pub(crate) bus: SystemBus<SystemBusMessage>,
    /// The notifications map
    pub(crate) notifications: OpenNotifications,
    /// The raft client
    pub(crate) raft: RaftClient,
}

/// A client interface to the raft
#[derive(Clone)]
pub struct RaftClient {
    /// The client's config
    pub(crate) config: RaftClientConfig,
    /// The inner raft
    raft: Raft,
    /// The network to use for the raft client
    network_factory: P2PNetworkFactoryWrapper,
    /// A lock to prevent simultaneous membership changes
    membership_change_lock: Arc<Mutex<()>>,
}

/// The config for the raft client
#[derive(Clone)]
pub struct RaftClientConfig {
    /// The id of the local node
    pub id: NodeId,
    /// Whether to initialize the cluster
    ///
    /// Initialization handles the process of setting up an initial set of nodes
    /// and running an initial election
    pub init: bool,
    /// The name of the cluster
    pub cluster_name: String,
    /// The interval in milliseconds between heartbeats
    pub heartbeat_interval: u64,
    /// The minimum election timeout in milliseconds
    pub election_timeout_min: u64,
    /// The maximum election timeout in milliseconds
    pub election_timeout_max: u64,
    /// The length of a learner's log lag before being promoted to a follower
    pub learner_promotion_threshold: u64,
    /// The directory at which snapshots are stored
    pub snapshot_path: String,
    /// The nodes to initialize the membership with
    pub initial_nodes: Vec<(NodeId, RaftNode)>,
    /// The maximum size of snapshot chunks in bytes
    pub snapshot_max_chunk_size: u64,
    /// The timeout on individual `InstallSnapshot` RPC calls
    pub install_snapshot_timeout: u64,
    /// The maximum number of log entries in an `AppendEntries` payload
    pub max_payload_entries: u64,
}

/// A client interface to the raft
#[derive(Clone)]
pub struct RaftClient {
    /// The client's config
    pub(crate) config: RaftClientConfig,
    /// The inner raft
    raft: Raft,
    /// The network to use for the raft client
    network_factory: P2PNetworkFactoryWrapper,
    /// A lock to prevent simultaneous membership changes
    membership_change_lock: Arc<Mutex<()>>,
}

/// A wrapper type allowing for default implementations of the network factory
/// traits, particularly the foreign `RaftNetworkFactory` trait
#[derive(Clone)]
pub struct P2PNetworkFactoryWrapper {
    /// The inner factory implementation
    inner: Arc<dyn P2PNetworkFactory>,
}

/// The network shim
#[derive(Clone)]
pub struct GossipNetwork {
    /// The target node for this instance
    target: NodeId,
    /// The target node info
    target_info: Node,
    /// A sender to the network manager's queue
    network_sender: NetworkManagerQueue,
}
#[async_trait]
impl P2PRaftNetwork for GossipNetwork {
}

/// The task queue type for the network manager
pub type NetworkManagerQueue = TokioSender<NetworkManagerJob>;

/// A wrapper trait for the `openraft` network factory
///
/// We define this trait to allow for p2p specific implementation as well as to
/// enable use as a trait object
pub trait P2PNetworkFactory: Send + Sync + 'static {
    /// Create a new p2p client
    fn new_p2p_client(&self, target: NodeId, target_info: Node) -> P2PRaftNetworkWrapper;
}

/// A generalization of the raft network trait that specifically allows for
/// point-to-point communication
///
/// We implement the general raft network trait for all types that fit this
/// signature by simply calling out to the p2p implementation
#[async_trait]
pub trait P2PRaftNetwork: 'static + Sync + Send {
    /// The target this client is sending requests to
    fn target(&self) -> NodeId;
    /// Send an request to the target node
    async fn send_request(
        &self,
        target: NodeId,
        request: RaftRequest,
    ) -> Result<RaftResponse, RPCError<NodeId, Node, RaftError<NodeId>>>;
}

/// A wrapper around the p2p raft network that allows for a default
/// `RaftNetwork` implementation and to hide generics from higher level
/// interfaces
pub struct P2PRaftNetworkWrapper {
    /// The inner p2p network
    inner: Box<dyn P2PRaftNetwork + Send + Sync>,
}
#+end_src

** lookup addr

#+begin_src rust
BehaviorJob::AddAddress(peer_id, addr) => {
    swarm.behaviour_mut().kademlia_dht.add_address(&peer_id, addr);
    Ok(())
},
BehaviorJob::RemovePeer(peer_id) => {
    swarm.behaviour_mut().kademlia_dht.remove_peer(&peer_id);
    Ok(())
},
BehaviorJob::LookupAddr(peer_id, sender) => {
    let addr = swarm
        .behaviour_mut()
        .handle_pending_outbound_connection(
            ConnectionId::new_unchecked(0),
            Some(peer_id),
            &[],
            Endpoint::Dialer,
        )
        .map_err(|_| NetworkManagerError::Network(ERR_NO_KNOWN_ADDR.to_string()))?;

    sender
        .send(addr)
        .map_err(|_| NetworkManagerError::SendInternal(ERR_SEND_INTERNAL.to_string()))
}
#+end_src


#+begin_quote
fn handle_pending_outbound_connection(
    &mut self,
    _connection_id: ConnectionId,
    _maybe_peer: Option<PeerId>,
    _addresses: &[Multiaddr],
    _effective_role: Endpoint,
) -> Result<Vec<Multiaddr>, ConnectionDenied>
Callback that is invoked for every outbound connection attempt.

We have access to:

The PeerId, if known. Remember that we can dial without a PeerId.
All addresses passed to DialOpts are passed in here too.
The effective Role of this peer in the dial attempt. Typically, this is set to Endpoint::Dialer except if we are attempting a hole-punch.
The ConnectionId identifying the future connection resulting from this dial, if successful.
Note that the addresses returned from this function are only used for dialing if WithPeerIdWithAddresses::extend_addresses_through_behaviour is set.

Any error returned from this function will immediately abort the dial attempt.


#+end_quote


** handle raft request from request response

#+begin_src rust
#[instrument(name = "handle_raft_request", skip_all, err, fields(req_type))]
pub(crate) async fn handle_raft_request(
    &self,
    req: RaftRequest,
) -> Result<RaftResponse, ReplicationV2Error> {
    backfill_trace_field("req_type", req.type_str());
    match req {
        RaftRequest::AppendEntries(req) => {
            let res = self.raft().append_entries(req).await?;
            Ok(RaftResponse::AppendEntries(res))
        },
        RaftRequest::Vote(req) => {
            let res = self.raft().vote(req).await?;
            Ok(RaftResponse::Vote(res))
        },
        RaftRequest::InstallSnapshot(req) => {
            let res = self.raft().install_snapshot(req).await?;
            Ok(RaftResponse::InstallSnapshot(Ok(res)))
        },
        RaftRequest::ForwardedProposal(req) => {
            self.propose_transition(req).await?;
            Ok(RaftResponse::Ack)
        },
    }
}
#+end_src

** renegade openraft implementation

*** P2PNetworkFactoryWrapper network definition

#+begin_src rust
pub struct P2PNetworkFactoryWrapper {
    /// The inner factory implementation
    inner: Arc<dyn P2PNetworkFactory>,
}
#+end_src

*** impl RaftNetworkFactory<TypeConfig> for P2PNetworkFactoryWrapper

#+begin_src rust
impl RaftNetworkFactory<TypeConfig> for P2PNetworkFactoryWrapper {
    type Network = P2PRaftNetworkWrapper;

    async fn new_client(&mut self, target: NodeId, target_info: &Node) -> Self::Network {
        self.inner.new_p2p_client(target, *target_info)
    }
}
#+end_src

*** P2PRaftNetworkWrapper definition

#+begin_src rust
pub struct P2PRaftNetworkWrapper {
    /// The inner p2p network
    inner: Box<dyn P2PRaftNetwork + Send + Sync>,
}
#+end_src

*** impl P2PRaftNetworkWrapper

#+begin_src rust
impl P2PRaftNetworkWrapper {
    /// Constructor
    pub fn new<N: P2PRaftNetwork>(inner: N) -> Self {
        Self { inner: Box::new(inner) }
    }
}

#[async_trait]
impl P2PRaftNetwork for P2PRaftNetworkWrapper {
    fn target(&self) -> NodeId {
        self.inner.target()
    }

    async fn send_request(
        &self,
        target: NodeId,
        request: RaftRequest,
    ) -> Result<RaftResponse, RPCError<NodeId, Node, RaftError<NodeId>>> {
        self.inner.send_request(target, request).await
    }
}
#+end_src

*** impl RaftNetwork<TypeConfig> for P2PRaftNetworkWrapper

#+begin_src rust
impl RaftNetwork<TypeConfig> for P2PRaftNetworkWrapper {
    async fn append_entries(
        &mut self,
        rpc: AppendEntriesRequest<TypeConfig>,
        _option: RPCOption,
    ) -> Result<AppendEntriesResponse<NodeId>, RPCError<NodeId, Node, RaftError<NodeId>>> {
        let target = self.inner.target();
        let req = RaftRequest::AppendEntries(rpc);
        self.inner.send_request(target, req).await.map(|resp| resp.into_append_entries())
    }

    async fn install_snapshot(
        &mut self,
        rpc: InstallSnapshotRequest<TypeConfig>,
        _option: RPCOption,
    ) -> Result<
        InstallSnapshotResponse<NodeId>,
        RPCError<NodeId, Node, RaftError<NodeId, InstallSnapshotError>>,
    > {
        let target = self.inner.target();
        let req = RaftRequest::InstallSnapshot(rpc);
        let res =
            self.inner.send_request(target, req).await.map(|resp| resp.into_install_snapshot());

        // Map the error to have the correct remote error type
        // We do this because the remote error type can only be reported by the remote
        // peer. Other error types are determined locally by the RPC handler
        if res.is_err() {
            let mapped_err = match res.err().unwrap() {
                RPCError::Network(e) => RPCError::Network(e),
                RPCError::PayloadTooLarge(e) => RPCError::PayloadTooLarge(e),
                RPCError::Timeout(e) => RPCError::Timeout(e),
                RPCError::Unreachable(e) => RPCError::Unreachable(e),
                _ => unreachable!("remote errors sent in response"),
            };

            return Err(mapped_err);
        };

        match res.unwrap() {
            Ok(resp) => Ok(resp),
            Err(e) => {
                let err = RaftError::APIError(e);
                Err(RPCError::RemoteError(RemoteError::new(target, err)))
            },
        }
    }

    async fn vote(
        &mut self,
        rpc: VoteRequest<NodeId>,
        _option: RPCOption,
    ) -> Result<VoteResponse<NodeId>, RPCError<NodeId, Node, RaftError<NodeId>>> {
        let target = self.inner.target();
        let req = RaftRequest::Vote(rpc);
        self.inner.send_request(target, req).await.map(|resp| resp.into_vote())
    }
}
#+end_src
